% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Project 1},
  pdfauthor={Silje Anfindsen},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering

\title{Project 1}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{TMA4315 GLM H2020}
\author{Silje Anfindsen}
\date{29 September, 2020}

\begin{document}
\maketitle

\hypertarget{problem-1}{%
\section{Problem 1}\label{problem-1}}

\hypertarget{a-glm-with-poisson-distributed-response-variable-and-canonical-link-function}{%
\subsection{1a) GLM with Poisson distributed response variable and
canonical link
function}\label{a-glm-with-poisson-distributed-response-variable-and-canonical-link-function}}

\hypertarget{poisson-distribution}{%
\subsubsection{Poisson distribution}\label{poisson-distribution}}

\[
f(y)=\frac{\lambda^y}{y!}e^{-\lambda} \text{ for } y=0,1,2,...
\]

\hypertarget{likelihood}{%
\subsubsection{likelihood}\label{likelihood}}

We assume that pairs of covariates and response are measured
independently of each other: \(({\bf x}_i,Y_i)\), and \(Y_i\) follows
the distribution specified above, and \({\bf x}_i\) is fixed.

\[L(\beta)=\prod_{i=1}^n L_i(\beta)=\prod_{i=1}^n f(y_i; \beta)=\prod_{i=1}^n\frac{\lambda_i^{y_i}}{y_i!}\exp(-\lambda_i)\]

\hypertarget{log-likelihood}{%
\subsubsection{log likelihood}\label{log-likelihood}}

\[l(\beta)=\ln L(\beta)=\sum_{i=1}^n \ln L_i(\beta)=\sum_{i=1}^n l_i(\beta)=\sum_{i=1}^n [y_i \ln(\lambda_i)-\lambda_i-\ln(y!)]\]

\hypertarget{score-function}{%
\subsubsection{Score function}\label{score-function}}

\[s(\beta)=\frac{\partial l(\beta)}{\partial \beta}=
\sum_{i=1}^n \frac{\partial l_i(\beta)}{\partial \beta}=
\sum_{i=1}^n s_i(\beta)
\] \[
s_i(\beta)=\frac{\partial l_i(\beta)}{\partial \beta}=\frac{\partial l_i(\beta)}{\partial \eta_i}\cdot \frac{\partial \eta_i}{\partial \beta}=\frac{\partial [y_i\eta_i-\exp(\eta_i)+C_i]}{\partial \eta_i}\cdot \frac{\partial [{\bf x}_i^T\beta ]}{\partial \beta} =[y_i-\exp(\eta_i)]\cdot {\bf x}_i=(y_i-\lambda_i){\bf x}_i \]

\[s(\beta)=\sum_{i=1}^n s_i(\beta)=\sum_{i=1}^n (y_i-\lambda_i){\bf x}_i\]

\hypertarget{fisher-information}{%
\subsubsection{Fisher information}\label{fisher-information}}

\$\$

\begin{align} F(\beta) &= \text{Cov}(s(\beta)) =   \sum_{i=1}^n \text{Cov}(s_i(\beta)) \\
&= \sum_{i=1}^n E\left[\Big(s_i(\beta) - E(s_i(\beta))\Big)\Big(s_i(\beta)-E(s_i(\beta))\Big)^T\right] \\
&= \sum_{i=1}^n E(s_i(\beta)s_i(\beta)^T) = \sum_{i=1}^n F_i(\beta) 
\end{align} \$\$

\[ F_i(\beta) = E(s_i(\beta)s_i(\beta)^T) = E((Y_i-\lambda_i){\bf x}_i(Y_i-\lambda_i){\bf x}_i^T) = {\bf x}_i{\bf x}_i^T E((Y_i-\lambda_i)^2) = {\bf x}_i{\bf x}_i^T \lambda_i \]
where \[E((Y_i-\lambda_i)^2)=\text{Var}(Y_i)=\lambda\] is the variance
of \(Y_i\)

\[F(\beta) = \sum_{i=1}^n {\bf x}_i{\bf x}_i^T \lambda_i.\]

\#\#1b) Fisher Scoring Algorithm

The expected Fisher information matrix \(F\) needs to be invertible. As
the parameters \(\lambda_i=e^{x_i^T\beta}>0\) for all \(i\) and the
design matrix \(X\) has full rank, \(p\) we know \(F\) is invertible.
With the linear link \(\lambda_i = \eta_i\) we set some restrictions:

Standard error estimates are the the square root of diagonal elements of
of the inverse Fisher information matrix

for the observation \(y_i=0\) and \(\hat{\lambda}=y_i=0\) (saturated
model)\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#input: model formula, data frame, start point}
\CommentTok{#output: coefficients, deviance, estimated variance of coefficients}
\NormalTok{myglm <-}\StringTok{ }\ControlFlowTok{function}\NormalTok{(formula, data, }\DataTypeTok{start=}\DecValTok{0}\NormalTok{) \{}
\NormalTok{  y <-}\StringTok{ }\NormalTok{data}\OperatorTok{$}\NormalTok{y}
\NormalTok{  X <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(formula, data) }\CommentTok{#design matrix}
\NormalTok{  beta <-}\StringTok{ }\KeywordTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\KeywordTok{ncol}\NormalTok{(X)) }\CommentTok{#beta vector}
  
  \ControlFlowTok{repeat}\NormalTok{ \{}
\NormalTok{    eta <-}\StringTok{ }\KeywordTok{as.vector}\NormalTok{(X}\OperatorTok{%*%}\StringTok{ }\NormalTok{beta) }\CommentTok{#eta vector}
\NormalTok{    lambda <-}\StringTok{ }\KeywordTok{exp}\NormalTok{(eta) }
\NormalTok{    W <-}\StringTok{ }\KeywordTok{diag}\NormalTok{(lambda) }\CommentTok{#diagonal matrix of lambdas}
\NormalTok{    score <-}\StringTok{ }\KeywordTok{apply}\NormalTok{((y}\OperatorTok{-}\NormalTok{lambda)}\OperatorTok{*}\NormalTok{X,}\DecValTok{2}\NormalTok{,sum)}
\NormalTok{    Fisherinfo <-}\StringTok{ }\KeywordTok{t}\NormalTok{(X) }\OperatorTok{%*%}\StringTok{ }\NormalTok{W }\OperatorTok{%*%}\StringTok{ }\NormalTok{X}
    
  \ControlFlowTok{if}\NormalTok{ (}\KeywordTok{sum}\NormalTok{(score}\OperatorTok{^}\DecValTok{2}\NormalTok{)}\OperatorTok{<}\FloatTok{1e-10}\NormalTok{) }\CommentTok{#break point}
    \ControlFlowTok{break}\NormalTok{()}
    
    \CommentTok{#find coefficient estimate and std.error}
\NormalTok{    beta <-}\StringTok{ }\NormalTok{beta }\OperatorTok{+}\StringTok{ }\KeywordTok{solve}\NormalTok{(Fisherinfo) }\OperatorTok{%*%}\StringTok{ }\NormalTok{score}
\NormalTok{    F.inv <-}\StringTok{ }\KeywordTok{solve}\NormalTok{(Fisherinfo)}
\NormalTok{    std.error <-}\StringTok{ }\KeywordTok{sqrt}\NormalTok{(}\KeywordTok{diag}\NormalTok{(F.inv))}
    \CommentTok{#matrix of beta and std.error:}
\NormalTok{    coeff =}\StringTok{ }\KeywordTok{cbind}\NormalTok{(beta, std.error)}
    \KeywordTok{colnames}\NormalTok{(coeff) <-}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"Estimate"}\NormalTok{, }\StringTok{"Std.Error"}\NormalTok{)}
\NormalTok{  \}}
  \CommentTok{# find deviance}
\NormalTok{  log.cand <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(y}\OperatorTok{*}\KeywordTok{log}\NormalTok{(lambda)}\OperatorTok{-}\NormalTok{lambda) }\CommentTok{#removed y as it cancels!}
\NormalTok{  yfix <-}\StringTok{ }\NormalTok{y }\CommentTok{#fix special case for saturated model when lambda=y=0}
\NormalTok{  yfix[y}\OperatorTok{==}\DecValTok{0}\NormalTok{] <-}\StringTok{ }\DecValTok{1} \CommentTok{#set log(0)=1}
\NormalTok{  log.sat <-}\StringTok{ }\KeywordTok{sum}\NormalTok{(yfix }\OperatorTok{*}\StringTok{ }\KeywordTok{log}\NormalTok{(yfix)}\OperatorTok{-}\NormalTok{yfix) }\CommentTok{#set y=lambda}
\NormalTok{  deviance =}\StringTok{ }\DecValTok{2} \OperatorTok{*}\StringTok{ }\KeywordTok{sum}\NormalTok{(log.sat }\OperatorTok{-}\StringTok{ }\NormalTok{log.cand)}
  
  \KeywordTok{return}\NormalTok{ (}\KeywordTok{list}\NormalTok{(}\DataTypeTok{coef=}\NormalTok{coeff, }\DataTypeTok{deviance=}\NormalTok{deviance, }\DataTypeTok{vcov =}\NormalTok{ F.inv))}
  
\NormalTok{ \}}
\end{Highlighting}
\end{Shaded}

\hypertarget{c-test}{%
\subsection{1c) Test}\label{c-test}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#model from ex.2}
\KeywordTok{load}\NormalTok{(}\KeywordTok{url}\NormalTok{(}\StringTok{"https://www.math.ntnu.no/emner/TMA4315/2020h/hoge-veluwe.Rdata"}\NormalTok{))}
\NormalTok{mod <-}\StringTok{ }\KeywordTok{myglm}\NormalTok{(data}\OperatorTok{$}\NormalTok{y}\OperatorTok{~}\NormalTok{t}\OperatorTok{+}\KeywordTok{I}\NormalTok{(t}\OperatorTok{^}\DecValTok{2}\NormalTok{), data)}
\NormalTok{mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $coef
##                 Estimate   Std.Error
## (Intercept)  1.420130462 0.282433135
## t            0.085183057 0.034053808
## I(t^2)      -0.003298608 0.001019461
## 
## $deviance
## [1] 249.4613
## 
## $vcov
##               (Intercept)            t        I(t^2)
## (Intercept)  0.0797684757 -0.009308502  0.0002550173
## t           -0.0093085017  0.001159662 -0.0000336900
## I(t^2)       0.0002550173 -0.000033690  0.0000010393
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#test code with glm function}
\NormalTok{testMod <-}\StringTok{ }\KeywordTok{glm}\NormalTok{(}\DataTypeTok{formula =}\NormalTok{ data}\OperatorTok{$}\NormalTok{y}\OperatorTok{~}\NormalTok{t}\OperatorTok{+}\KeywordTok{I}\NormalTok{(t}\OperatorTok{^}\DecValTok{2}\NormalTok{), }\DataTypeTok{data =}\NormalTok{ data, }\DataTypeTok{family =} \KeywordTok{poisson}\NormalTok{(}\DataTypeTok{link =}\NormalTok{ log))}
\KeywordTok{summary}\NormalTok{(testMod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## glm(formula = data$y ~ t + I(t^2), family = poisson(link = log), 
##     data = data)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -3.7797  -0.7788   0.3430   0.7438   2.2312  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(>|z|)    
## (Intercept)  1.420130   0.282427   5.028 4.95e-07 ***
## t            0.085183   0.034053   2.502  0.01237 *  
## I(t^2)      -0.003299   0.001019  -3.236  0.00121 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 300.11  on 134  degrees of freedom
## Residual deviance: 277.46  on 132  degrees of freedom
## AIC: 740.67
## 
## Number of Fisher Scoring iterations: 5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{vcov}\NormalTok{(testMod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               (Intercept)             t        I(t^2)
## (Intercept)  0.0797650582 -9.307971e-03  2.549981e-04
## t           -0.0093079707  1.159579e-03 -3.368697e-05
## I(t^2)       0.0002549981 -3.368697e-05  1.039189e-06
\end{verbatim}

\hypertarget{problem-2}{%
\section{Problem 2}\label{problem-2}}

\hypertarget{a-interpret-parameters}{%
\subsection{2a) Interpret parameters}\label{a-interpret-parameters}}

Gaussian function for the expected number of fledglings, \(\lambda_i\),
produced by each female and dependent on \(t_i\).

\[\lambda_i = \lambda_0 \exp( -\frac{(t_i-\theta)^2}{2 \omega^2})\]
Interpret parameters in the function above:

\(\lambda_0\) is the height of the curve's peak, which in this example
is the maximum number of fledglings leaving the nest, \(y_{max}\).

\(\theta\) is the position of the center of the peak, in other words the
breeding time for first egg giving a maximum number of fledglings laving
the nest, \(t_{max}\)

\$\omega \$ controls the width of the ``bell'', which can be interpreted
as the spread or variance of the number of fledglings leaving the nest.

\hypertarget{b}{%
\subsection{2b)}\label{b}}

\hypertarget{glm}{%
\subsubsection{GLM}\label{glm}}

\hypertarget{relations-between-glm-parameters-contained-in-beta-and-_0}{%
\subsubsection{\texorpdfstring{relations between GLM parameters
contained in \(\beta\) and (\lambda\_0, \theta,
\omega)}{relations between GLM parameters contained in \textbackslash beta and (\_0, , )}}\label{relations-between-glm-parameters-contained-in-beta-and-_0}}

\$\$

\log (\lambda\_i) = \log (\lambda\_0) -\frac{(t_i-\theta)^2}{2 \omega^2}
= \log (\lambda\_0) - \frac{t_i^2}{2\omega} + \frac{t_i \theta}{\omega}
- \frac{\theta}{2\omega}

\$\$ Recall

\[
y(t_i) = \beta_0 + \beta_1t_i + \beta_2 t_i^2
\] This gives the following coefficients:

\[
\beta_0 = \log(\lambda_0) -\frac{\theta^2}{2\omega^2}\\ 
\beta_1 = \frac{\theta}{\omega^2}\\ 
\beta_2 = -\frac{1}{2\omega^2}
\] This gives the following model:

\[
y(t_i) = \log(\lambda_0) -\frac{\theta^2}{2\omega^2} + \frac{\theta}{\omega^2}t_i -\frac{1}{2\omega^2} t_i^2
\]

\hypertarget{arguments-why-this-is-a-glm}{%
\subsubsection{arguments why this is a
GLM}\label{arguments-why-this-is-a-glm}}

From exercise 1 we know that MLE for \(\beta\) is \$\beta = \$

Spor om denne oppgaven!

\hypertarget{c-fit-the-model-using-the-function-in-problem-1b}{%
\subsection{2c) Fit the model using the function in problem
1b}\label{c-fit-the-model-using-the-function-in-problem-1b}}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#model from ex.2}
\KeywordTok{load}\NormalTok{(}\KeywordTok{url}\NormalTok{(}\StringTok{"https://www.math.ntnu.no/emner/TMA4315/2020h/hoge-veluwe.Rdata"}\NormalTok{))}
\NormalTok{mod <-}\StringTok{ }\KeywordTok{myglm}\NormalTok{(data}\OperatorTok{$}\NormalTok{y}\OperatorTok{~}\NormalTok{t}\OperatorTok{+}\KeywordTok{I}\NormalTok{(t}\OperatorTok{^}\DecValTok{2}\NormalTok{), data)}
\NormalTok{mod}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $coef
##                 Estimate   Std.Error
## (Intercept)  1.420130462 0.282433135
## t            0.085183057 0.034053808
## I(t^2)      -0.003298608 0.001019461
## 
## $deviance
## [1] 249.4613
## 
## $vcov
##               (Intercept)            t        I(t^2)
## (Intercept)  0.0797684757 -0.009308502  0.0002550173
## t           -0.0093085017  0.001159662 -0.0000336900
## I(t^2)       0.0002550173 -0.000033690  0.0000010393
\end{verbatim}

\hypertarget{d-hypothesis-test-for-quadratic-effect-of-t}{%
\subsection{2d) hypothesis test for quadratic effect of
t}\label{d-hypothesis-test-for-quadratic-effect-of-t}}

H0: \(\beta_2 = 0\) H1: \(\beta_2 \neq 0\)

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#likelihood ratio test: difference between two deviances}

\CommentTok{#hypothesis test using LRT, comparing deviances}
\NormalTok{modA <-}\StringTok{ }\KeywordTok{myglm}\NormalTok{(data}\OperatorTok{$}\NormalTok{y}\OperatorTok{~}\NormalTok{t}\OperatorTok{+}\KeywordTok{I}\NormalTok{(t}\OperatorTok{^}\DecValTok{2}\NormalTok{), data)}
\NormalTok{modB <-}\StringTok{ }\KeywordTok{myglm}\NormalTok{(data}\OperatorTok{$}\NormalTok{y}\OperatorTok{~}\NormalTok{t, data)}

\NormalTok{df =}\StringTok{ }\KeywordTok{length}\NormalTok{(modA}\OperatorTok{$}\NormalTok{coef) }\OperatorTok{-}\StringTok{ }\KeywordTok{length}\NormalTok{(modB}\OperatorTok{$}\NormalTok{coef)}

\NormalTok{deviance <-}\StringTok{ }\NormalTok{modB}\OperatorTok{$}\NormalTok{deviance}\OperatorTok{-}\NormalTok{modA}\OperatorTok{$}\NormalTok{deviance}
\NormalTok{Xp =}\StringTok{ }\KeywordTok{qchisq}\NormalTok{(}\FloatTok{0.95}\NormalTok{, df)}

\NormalTok{Xp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5.991465
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{deviance}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 11.92985
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#eventuelt regn ut p-verdien og se at den er mindre enn significant level}
\NormalTok{pvalue =}\StringTok{ }\DecValTok{1}\OperatorTok{-}\KeywordTok{pchisq}\NormalTok{(deviance, df)}
\NormalTok{pvalue}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.002567235
\end{verbatim}

Since D\textgreater Xp we reject H0 and the quadratic effect of t is
significant.

\hypertarget{e-goodness-of-fit}{%
\subsection{2e) Goodness-of-fit}\label{e-goodness-of-fit}}

Using the observed deviance, test the goodness-of-fit of the fitted
model. How well does the model fit the data?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{#deviance test for model, pvalue }
\DecValTok{1} \OperatorTok{-}\StringTok{ }\KeywordTok{pchisq}\NormalTok{(mod}\OperatorTok{$}\NormalTok{deviance, (}\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{t)}\OperatorTok{-}\KeywordTok{length}\NormalTok{(mod}\OperatorTok{$}\NormalTok{coef)) )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.064435e-09
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{D =}\StringTok{ }\NormalTok{mod}\OperatorTok{$}\NormalTok{deviance}
\NormalTok{Xp =}\StringTok{ }\KeywordTok{qchisq}\NormalTok{(}\FloatTok{0.95}\NormalTok{, (}\KeywordTok{length}\NormalTok{(data}\OperatorTok{$}\NormalTok{t)}\OperatorTok{-}\KeywordTok{length}\NormalTok{(mod}\OperatorTok{$}\NormalTok{coef)))}
\NormalTok{Xp}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 156.5075
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{D}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 249.4613
\end{verbatim}

The model fit is good

Examine the distribution of the y more closely and discuss how the
assumptions of the model may be violated.

\hypertarget{problem-3}{%
\section{Problem 3}\label{problem-3}}

Whereas nonparametric bootstraps make no assumptions about how your
observations are distributed, and resample your original sample,
parametric bootstraps resample a known distribution function, whose
parameters are estimated from your sample.

\end{document}
